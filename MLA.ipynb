{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC0a0RhzPUFJ",
        "outputId": "42f69a4a-e0f1-4799-be71-96fda5244354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxCQ4o5gPExW",
        "outputId": "e633c490-8b5e-46d3-cb08-121105b3ee64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-0d5fc762c9a9>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 10.3743\n",
            "Epoch 1 Loss: 10.3709\n",
            "Epoch 2 Loss: 10.3212\n",
            "Epoch 3 Loss: 10.2011\n",
            "Epoch 4 Loss: 10.0721\n",
            "Epoch 5 Loss: 10.0883\n",
            "Epoch 6 Loss: 9.8448\n",
            "Epoch 7 Loss: 9.8917\n",
            "Epoch 8 Loss: 9.9292\n",
            "Epoch 9 Loss: 10.1116\n",
            "Epoch 10 Loss: 9.7340\n",
            "Epoch 11 Loss: 9.6669\n",
            "Epoch 12 Loss: 9.7451\n",
            "Epoch 13 Loss: 9.9550\n",
            "Epoch 14 Loss: 9.4845\n",
            "Epoch 15 Loss: 9.7672\n",
            "Epoch 16 Loss: 9.7179\n",
            "Epoch 17 Loss: 9.5093\n",
            "Epoch 18 Loss: 9.4009\n",
            "Epoch 19 Loss: 9.4106\n",
            "Epoch 20 Loss: 9.3038\n",
            "Epoch 21 Loss: 9.3128\n",
            "Epoch 22 Loss: 9.4516\n",
            "Epoch 23 Loss: 9.2486\n",
            "Epoch 24 Loss: 9.1545\n",
            "Epoch 25 Loss: 9.3491\n",
            "Epoch 26 Loss: 9.3559\n",
            "Epoch 27 Loss: 9.2436\n",
            "Epoch 28 Loss: 9.2613\n",
            "Epoch 29 Loss: 9.1063\n",
            "Epoch 30 Loss: 9.2779\n",
            "Epoch 31 Loss: 9.2745\n",
            "Epoch 32 Loss: 9.0411\n",
            "Epoch 33 Loss: 9.1837\n",
            "Epoch 34 Loss: 9.1079\n",
            "Epoch 35 Loss: 9.0109\n",
            "Epoch 36 Loss: 9.2038\n",
            "Epoch 37 Loss: 9.2967\n",
            "Epoch 38 Loss: 9.0990\n",
            "Epoch 39 Loss: 9.0388\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import math\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 32000\n",
        "        self.d_model = 5120\n",
        "        self.n_layers = 2\n",
        "        self.n_heads = 8\n",
        "        self.d_kv_comp = 128\n",
        "        self.d_rope = 16\n",
        "        self.n_experts = 32\n",
        "        self.n_shared = 2\n",
        "        self.top_k = 2\n",
        "        self.seq_len = 256\n",
        "        self.batch_size = 1\n",
        "        self.ffn_dim = 384\n",
        "        self.device_groups = 4 # For device-limited routing\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config.d_model, config.ffn_dim)\n",
        "        self.w2 = nn.Linear(config.ffn_dim, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, scale=40):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"Dimension must be even for rotary embeddings\"\n",
        "        self.dim = dim\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim//2, 2).float() / (dim//2)))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.scale = 40\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq) / self.scale\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        return torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary(x, cos, sin):\n",
        "    \"\"\"\n",
        "    Apply rotary embeddings to the first half of x.\n",
        "    \"\"\"\n",
        "    # Split x into two parts: one for rotary embeddings and the other untouched\n",
        "    x_rot, x_base = x.split(cos.shape[-1], dim=-1)\n",
        "    # Apply rotary embeddings to the rotary part\n",
        "    x_rot = (x_rot * cos) + (rotate_half(x_rot) * sin)\n",
        "    # Concatenate the rotary-applied and base parts\n",
        "    return torch.cat([x_rot, x_base], dim=-1)\n",
        "\n",
        "\n",
        "class MemoryOptimizedMLA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d_head = config.d_model // config.n_heads\n",
        "        self.split_dim = self.d_head - config.d_rope\n",
        "\n",
        "        # Projections\n",
        "        self.W_dkv = nn.Linear(config.d_model, config.d_kv_comp)\n",
        "        self.W_dq = nn.Linear(config.d_model, config.d_kv_comp)\n",
        "\n",
        "        # Changed value projection to use d_head instead of split_dim\n",
        "        self.W_uk = nn.Linear(config.d_kv_comp, config.n_heads * self.split_dim)\n",
        "        self.W_uv = nn.Linear(config.d_kv_comp, config.n_heads * self.d_head)\n",
        "        self.W_uq = nn.Linear(config.d_kv_comp, config.n_heads * self.split_dim)\n",
        "\n",
        "        self.W_qr = nn.Linear(config.d_kv_comp, config.n_heads * config.d_rope)\n",
        "        self.W_kr = nn.Linear(config.d_model, config.n_heads * config.d_rope)\n",
        "\n",
        "        self.rotary = RotaryEmbedding(config.d_rope)\n",
        "        self.output = nn.Linear(config.n_heads * self.d_head, config.d_model)\n",
        "\n",
        "    def forward(self, h, past_kv=None):\n",
        "        batch_size, seq_len, _ = h.shape\n",
        "\n",
        "        # KV Compression\n",
        "        c_kv = self.W_dkv(h)\n",
        "        k = self.W_uk(c_kv).view(batch_size, seq_len, config.n_heads, self.split_dim)\n",
        "        v = self.W_uv(c_kv).view(batch_size, seq_len, config.n_heads, self.d_head)\n",
        "\n",
        "        # Query Compression\n",
        "        c_q = self.W_dq(h)\n",
        "        q_base = self.W_uq(c_q).view(batch_size, seq_len, config.n_heads, self.split_dim)\n",
        "        q_rot = self.W_qr(c_q).view(batch_size, seq_len, config.n_heads, config.d_rope)\n",
        "\n",
        "        # Rotary embeddings with proper dimensions\n",
        "        rotary_emb = self.rotary(seq_len)\n",
        "        cos = torch.cos(rotary_emb).view(1, seq_len, 1, -1)  # [1, seq, 1, dim]\n",
        "        sin = torch.sin(rotary_emb).view(1, seq_len, 1, -1)\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        q_rot = apply_rotary(q_rot, cos, sin)\n",
        "        k_rot = apply_rotary(\n",
        "            self.W_kr(h).view(batch_size, seq_len, config.n_heads, config.d_rope),\n",
        "            cos, sin\n",
        "        )\n",
        "\n",
        "        q = torch.cat([q_base, q_rot], dim=-1)\n",
        "        k = torch.cat([k, k_rot], dim=-1)\n",
        "\n",
        "        # Attention computation\n",
        "        scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(self.d_head)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = torch.einsum(\"bhqk,bkhd->bqhd\", attn, v)\n",
        "\n",
        "        return self.output(out.contiguous().view(batch_size, seq_len, -1)), (c_kv, k_rot)\n",
        "\n",
        "\n",
        "class DeepSeekMoE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.shared_experts = nn.ModuleList([Expert() for _ in range(config.n_shared)])\n",
        "        self.routed_experts = nn.ModuleList([Expert() for _ in range(config.n_experts)])\n",
        "        self.gate = nn.Linear(config.d_model, config.n_experts)\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared experts process all tokens\n",
        "        shared_out = sum(expert(x) for expert in self.shared_experts)\n",
        "\n",
        "        # Device-limited routing\n",
        "        routed_logits = self.gate(x)\n",
        "        probs = F.softmax(routed_logits, dim=-1)\n",
        "        topk_probs, topk_indices = probs.topk(config.top_k, dim=-1)\n",
        "\n",
        "        # Expert balance loss\n",
        "        expert_counts = torch.zeros(config.n_experts, device=x.device)\n",
        "        expert_counts.scatter_add_(0, topk_indices.view(-1),\n",
        "                                 torch.ones_like(topk_indices.view(-1), dtype=torch.float))\n",
        "        self.aux_loss += expert_counts.float().var() * 0.003  # α1 from paper\n",
        "\n",
        "        # Sparse computation\n",
        "        routed_out = torch.zeros_like(x)\n",
        "        for k in range(config.top_k):\n",
        "            expert_mask = topk_indices[..., k]\n",
        "            expert_contrib = torch.zeros_like(x)\n",
        "\n",
        "            for expert_idx in range(config.n_experts):\n",
        "                mask = (expert_mask == expert_idx)\n",
        "                if mask.any():\n",
        "                    expert_out = self.routed_experts[expert_idx](x[mask])\n",
        "                    expert_contrib[mask] = expert_out * topk_probs[..., k][mask].unsqueeze(-1)\n",
        "\n",
        "            routed_out += expert_contrib\n",
        "\n",
        "        return shared_out + routed_out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = MemoryOptimizedMLA()\n",
        "        self.norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.moe = DeepSeekMoE()\n",
        "\n",
        "    def forward(self, x, past_kv=None):\n",
        "        # Attention with KV cache\n",
        "        attn_out, new_kv = checkpoint(self.attn, self.norm1(x), past_kv)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # MoE with checkpointing\n",
        "        moe_out = checkpoint(self.moe, self.norm2(x))\n",
        "        x = x + moe_out\n",
        "\n",
        "        return x, new_kv\n",
        "\n",
        "class DeepSeekV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock() for _ in range(config.n_layers)])\n",
        "        self.norm = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "        # Better initialization with residual scaling\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_normal_(p, gain=0.1/math.sqrt(config.n_layers))\n",
        "        # Add residual scaling\n",
        "        for block in self.blocks:\n",
        "            block.attn.output.weight.data.mul_(0.1)\n",
        "            block.moe.shared_experts[0].w2.weight.data.mul_(0.1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids)\n",
        "        total_aux_loss = 0.0\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x, _ = block(x)\n",
        "            total_aux_loss += block.moe.aux_loss\n",
        "\n",
        "        return self.lm_head(self.norm(x)), total_aux_loss\n",
        "\n",
        "def train(epochs):\n",
        "    model = DeepSeekV2().cuda()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
        "\n",
        "    # Learning rate schedule with warmup\n",
        "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=3e-4,  # Increased max LR\n",
        "        total_steps=epochs,\n",
        "        pct_start=0.1,  # Shorter warmup\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Use structured inputs\n",
        "        inputs = torch.randint(0, config.vocab_size // 10,\n",
        "                               (config.batch_size, config.seq_len + 1)).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits, aux_loss = model(inputs[:, :-1])\n",
        "            loss = F.cross_entropy(logits.view(-1, config.vocab_size),\n",
        "                                   inputs[:, 1:].contiguous().view(-1))\n",
        "            loss += 0.0001 * aux_loss\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okpwkF-tPZA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}